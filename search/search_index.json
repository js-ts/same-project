{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"","title":"Home"},{"location":"blog/_index/","text":"","title":" index"},{"location":"blog/articles/_index/","text":"","title":" index"},{"location":"community/adopters/","text":"Adopters of SAME \u00b6","title":"Adopters of SAME"},{"location":"community/adopters/#adopters-of-same","text":"","title":"Adopters of SAME"},{"location":"community/presentations/","text":"SAME \u00b6","title":"SAME"},{"location":"community/presentations/#same","text":"","title":"SAME"},{"location":"developer/debug/","text":"SAME Debugging Guide \u00b6","title":"SAME Debugging Guide"},{"location":"developer/debug/#same-debugging-guide","text":"","title":"SAME Debugging Guide"},{"location":"developer/developer/","text":"","title":"Developer"},{"location":"getting-started/_index/","text":"","title":"How to Get Started with SAME"},{"location":"getting-started/adding-steps/","text":"In order to add steps, we need a comment in the notebook itself. This can be done by adding a tag in the notebook, or a comment in the python file. Allowing for Adding Specific Tags \u00b6 First, go to the notebook settings and view \"Cell Metadata\": Add a Tag that Specifies a Step \u00b6 Second, go to the cell you want to split steps with, and add a tag of the following form: same_step_x where x is a digit of any length. (We currently only support linear DAG execution so even though we ask for digits, we don't actually do anything with them.) Execute the step \u00b6 Now, when you execute same program run the cells will automatically be grouped together into steps, and executed serially. No additional work is necessary for the end user. We inject two additional steps for each graph, a \"run info\" step (which is necessary because we only know things like Run ID when the step is executed), and the \"context\" step (which adds a default, well-formed context dictionary to the graph).","title":"Adding Steps"},{"location":"getting-started/adding-steps/#allowing-for-adding-specific-tags","text":"First, go to the notebook settings and view \"Cell Metadata\":","title":"Allowing for Adding Specific Tags"},{"location":"getting-started/adding-steps/#add-a-tag-that-specifies-a-step","text":"Second, go to the cell you want to split steps with, and add a tag of the following form: same_step_x where x is a digit of any length. (We currently only support linear DAG execution so even though we ask for digits, we don't actually do anything with them.)","title":"Add a Tag that Specifies a Step"},{"location":"getting-started/adding-steps/#execute-the-step","text":"Now, when you execute same program run the cells will automatically be grouped together into steps, and executed serially. No additional work is necessary for the end user. We inject two additional steps for each graph, a \"run info\" step (which is necessary because we only know things like Run ID when the step is executed), and the \"context\" step (which adds a default, well-formed context dictionary to the graph).","title":"Execute the step"},{"location":"getting-started/changing-environment/","text":"Often times, a specific step will require a particular base image to execute inside of. This may be because the image was pre-built with particular dependencies (packages, libraries, drivers, etc) which Jupyter does not need to execute, but may be necessary on the back end. This feature allows to specify the base image required to execute a given step. Editing the Notebook to Allow Tags \u00b6 First, go to the notebook settings and view \"Cell Metadata\": Adding an Environment Specifier \u00b6 You can add alternate base images with the following tag structure: environment = environment_name For example: environment = default or: environment = private-training-env Update SAME File \u00b6 Then in your SAME file, you'll add a section that maps to the specific image you'll need.","title":"Changing Environment"},{"location":"getting-started/changing-environment/#editing-the-notebook-to-allow-tags","text":"First, go to the notebook settings and view \"Cell Metadata\":","title":"Editing the Notebook to Allow Tags"},{"location":"getting-started/changing-environment/#adding-an-environment-specifier","text":"You can add alternate base images with the following tag structure: environment = environment_name For example: environment = default or: environment = private-training-env","title":"Adding an Environment Specifier"},{"location":"getting-started/changing-environment/#update-same-file","text":"Then in your SAME file, you'll add a section that maps to the specific image you'll need.","title":"Update SAME File"},{"location":"getting-started/dev-build/","text":"If you are using the same-mono-private repo, it does not currently produce a binary that can be installed from https://get.sameproject.org/. You will need to clone the repo and run the CLI from the main branch: Prerequisites \u00b6 Python 3.8 . _Note that 3.9 is not currently supported due to Azure Machine Learning dependencies. Poetry 1.1.7 or higher. Azure CLI 2.27 or higher. Azure Functions Work Tools v3.x or higher (Optional). Clone the repo to your local machine and initialize the submodules: git clone https://github.com/SAME-Project/same-project.git cd same-project git submodule update --init --recursive Download and install Poetry, which is used to manage dependencies and virtual environments for the SAME project. You will need to install the project's Python dependencies using Poetry as well after installing it: curl -sSL https://raw.githubusercontent.com/python-poetry/poetry/master/install-poetry.py | python3 - poetry install To install AML dependencies, now optional, use poetry install --extras azureml Using the devcontainer with Visual Studio Code \u00b6 If you are using Visual Studio Code (VSCode) to work with the same-mono-private repo, VSCode supports development in a containerized environment through its Remote - Container extension , so you don't need to manually install all of the tools and frameworks yourself. Prerequisites \u00b6 Docker For Windows users, we recommend enabling WSL2 back-end integration with Docker . Visual Studio Code Visual Studio Code Remote - Containers extension Opening same-project in a devcontainer \u00b6 After you have cloned the dapr repo locally, open the dapr folder in VSCode. For example: git clone https://github.com/SAME-Project/same-project.git cd same-project git submodule update --init --recursive code . VSCode will detect the presence of a dev container definition in the repo and will prompt you to reopen the project in a container. Alternatively, you can open the command palette and use the Remote-Containers: Reopen in Container command. Once the container is loaded, open an integrated terminal in VSCode and you're ready to use the repo. You will still need to install the project Python dependencies using the preinstalled Poetry tool: poetry install To install AML dependencies, now optional, use poetry install --extras azureml Using the repo \u00b6 Use of the SAME python project assumes executing in a virtual environment managed by Poetry. Before running any commands, the virtual environment should be started: poetry shell NOTE: From this point forward, all functions require executing inside that virtual environment. If you see an error like zsh: command not found , it could be because you're not executing inside the venv. You can check this by executing: which python3 This should result in a response like: .../pypoetry/virtualenvs/same-project-88mixeKa-py3.8/bin/python3 . If it reports something like /usr/bin/python or /usr/local/bin/python , you are using the system python, and things will not work as expected. How to execute against a notebook from source code \u00b6 From the root of project, execute: python3 cli/same/main.py <cli-arguments> TODO: Enable building the CLI into a redistributable binary via something like PyOxidiser in same-project. When we get to binary builds of the CLI that can be run locally, you can execute the local build with: bin/same <cli-arguments> After we start publishing builds, you can install and execute with the following: curl -L0 https://get.sameproject.ml/ | bash - same <cli-arguments> Running tests \u00b6 To run all the tests against the CLI and SDK: pytest To run a subset of tests for a single file: pytest test/cli/test_<file>.py -k \"test_<name>\" How to setup private test environments \u00b6 Local Kubeflow cluster on Minikube in devcontainer \u00b6 The devcontainer image for same-project comes with minikube preinstalled, so you can set up a local Kubeflow cluster to run the CLI pytests against if you wish: Start a minikube cluster in the devcontainer: Note: Kubeflow currently defines its Custom Resource Definitions (CRD) under apiextensions.k8s.io/v1beta which is deprecated in Kubernetes v1.22, so minikube must start the cluster with a version <1.22. See kubeflow/kfctl issue #500 . minikube start --kubernetes-version = v1.21.5 Starting minikube will also change the default kubeconfig context to the minikube cluster. You can check this with: kubectl config get-contexts Deploy Kubeflow to the minikube cluster: export PIPELINE_VERSION = 1 .7.0 kubectl apply -k \"github.com/kubeflow/pipelines/manifests/kustomize/cluster-scoped-resources?ref= $PIPELINE_VERSION \" kubectl wait --for condition = established --timeout = 60s crd/applications.app.k8s.io kubectl apply -k \"github.com/kubeflow/pipelines/manifests/kustomize/env/platform-agnostic-pns?ref= $PIPELINE_VERSION \" Kubeflow cluster on Azure Kubernetes Services (AKS) \u00b6 From any Azure subscription where you are at least a Contributor, you can create and provision a new AKS cluster with Kubeflow: Create a new AKS cluster either using the Azure CLI or Azure Portal . The linked instructions will also update your kubeconfig to use the new cluster as the context when you run az aks get-credentials , but you can also manually do so with: kubectl config set-context <context name> Deploy Kubeflow to the cluster. Note: The document references a non-existent v1.3.0 release, you can simply use the v1.2.0 release instead. See kubeflow/kfctl issue #495 . Azure Machine Learning (AML) workspace and compute \u00b6 Create a new Service Principal for running tests against your private AML instance. As mentioned in the instructions, make sure to take note of the output of the command as you will need the clientId , clientSecret , and tenantId values to configure the .env.sh file to run the AML tests. Create a new Azure Machine Learning Workspace . You will need the --resource-group and --workspace-name values you specified during workspace creation to configure the .env.sh file to run the AML tests. You will also need the subscription id that you created the AML workspace in. You can check this by running: az account show --query id Create an AML Compute cluster or AML Compute Instance . You will need the --name that you specified during compute cluster/instance creation to configure the .env.sh file to run the AML tests.","title":"Dev Build"},{"location":"getting-started/dev-build/#prerequisites","text":"Python 3.8 . _Note that 3.9 is not currently supported due to Azure Machine Learning dependencies. Poetry 1.1.7 or higher. Azure CLI 2.27 or higher. Azure Functions Work Tools v3.x or higher (Optional). Clone the repo to your local machine and initialize the submodules: git clone https://github.com/SAME-Project/same-project.git cd same-project git submodule update --init --recursive Download and install Poetry, which is used to manage dependencies and virtual environments for the SAME project. You will need to install the project's Python dependencies using Poetry as well after installing it: curl -sSL https://raw.githubusercontent.com/python-poetry/poetry/master/install-poetry.py | python3 - poetry install To install AML dependencies, now optional, use poetry install --extras azureml","title":"Prerequisites"},{"location":"getting-started/dev-build/#using-the-devcontainer-with-visual-studio-code","text":"If you are using Visual Studio Code (VSCode) to work with the same-mono-private repo, VSCode supports development in a containerized environment through its Remote - Container extension , so you don't need to manually install all of the tools and frameworks yourself.","title":"Using the devcontainer with Visual Studio Code"},{"location":"getting-started/dev-build/#prerequisites_1","text":"Docker For Windows users, we recommend enabling WSL2 back-end integration with Docker . Visual Studio Code Visual Studio Code Remote - Containers extension","title":"Prerequisites"},{"location":"getting-started/dev-build/#opening-same-project-in-a-devcontainer","text":"After you have cloned the dapr repo locally, open the dapr folder in VSCode. For example: git clone https://github.com/SAME-Project/same-project.git cd same-project git submodule update --init --recursive code . VSCode will detect the presence of a dev container definition in the repo and will prompt you to reopen the project in a container. Alternatively, you can open the command palette and use the Remote-Containers: Reopen in Container command. Once the container is loaded, open an integrated terminal in VSCode and you're ready to use the repo. You will still need to install the project Python dependencies using the preinstalled Poetry tool: poetry install To install AML dependencies, now optional, use poetry install --extras azureml","title":"Opening same-project in a devcontainer"},{"location":"getting-started/dev-build/#using-the-repo","text":"Use of the SAME python project assumes executing in a virtual environment managed by Poetry. Before running any commands, the virtual environment should be started: poetry shell NOTE: From this point forward, all functions require executing inside that virtual environment. If you see an error like zsh: command not found , it could be because you're not executing inside the venv. You can check this by executing: which python3 This should result in a response like: .../pypoetry/virtualenvs/same-project-88mixeKa-py3.8/bin/python3 . If it reports something like /usr/bin/python or /usr/local/bin/python , you are using the system python, and things will not work as expected.","title":"Using the repo"},{"location":"getting-started/dev-build/#how-to-execute-against-a-notebook-from-source-code","text":"From the root of project, execute: python3 cli/same/main.py <cli-arguments> TODO: Enable building the CLI into a redistributable binary via something like PyOxidiser in same-project. When we get to binary builds of the CLI that can be run locally, you can execute the local build with: bin/same <cli-arguments> After we start publishing builds, you can install and execute with the following: curl -L0 https://get.sameproject.ml/ | bash - same <cli-arguments>","title":"How to execute against a notebook from source code"},{"location":"getting-started/dev-build/#running-tests","text":"To run all the tests against the CLI and SDK: pytest To run a subset of tests for a single file: pytest test/cli/test_<file>.py -k \"test_<name>\"","title":"Running tests"},{"location":"getting-started/dev-build/#how-to-setup-private-test-environments","text":"","title":"How to setup private test environments"},{"location":"getting-started/dev-build/#local-kubeflow-cluster-on-minikube-in-devcontainer","text":"The devcontainer image for same-project comes with minikube preinstalled, so you can set up a local Kubeflow cluster to run the CLI pytests against if you wish: Start a minikube cluster in the devcontainer: Note: Kubeflow currently defines its Custom Resource Definitions (CRD) under apiextensions.k8s.io/v1beta which is deprecated in Kubernetes v1.22, so minikube must start the cluster with a version <1.22. See kubeflow/kfctl issue #500 . minikube start --kubernetes-version = v1.21.5 Starting minikube will also change the default kubeconfig context to the minikube cluster. You can check this with: kubectl config get-contexts Deploy Kubeflow to the minikube cluster: export PIPELINE_VERSION = 1 .7.0 kubectl apply -k \"github.com/kubeflow/pipelines/manifests/kustomize/cluster-scoped-resources?ref= $PIPELINE_VERSION \" kubectl wait --for condition = established --timeout = 60s crd/applications.app.k8s.io kubectl apply -k \"github.com/kubeflow/pipelines/manifests/kustomize/env/platform-agnostic-pns?ref= $PIPELINE_VERSION \"","title":"Local Kubeflow cluster on Minikube in devcontainer"},{"location":"getting-started/dev-build/#kubeflow-cluster-on-azure-kubernetes-services-aks","text":"From any Azure subscription where you are at least a Contributor, you can create and provision a new AKS cluster with Kubeflow: Create a new AKS cluster either using the Azure CLI or Azure Portal . The linked instructions will also update your kubeconfig to use the new cluster as the context when you run az aks get-credentials , but you can also manually do so with: kubectl config set-context <context name> Deploy Kubeflow to the cluster. Note: The document references a non-existent v1.3.0 release, you can simply use the v1.2.0 release instead. See kubeflow/kfctl issue #495 .","title":"Kubeflow cluster on Azure Kubernetes Services (AKS)"},{"location":"getting-started/dev-build/#azure-machine-learning-aml-workspace-and-compute","text":"Create a new Service Principal for running tests against your private AML instance. As mentioned in the instructions, make sure to take note of the output of the command as you will need the clientId , clientSecret , and tenantId values to configure the .env.sh file to run the AML tests. Create a new Azure Machine Learning Workspace . You will need the --resource-group and --workspace-name values you specified during workspace creation to configure the .env.sh file to run the AML tests. You will also need the subscription id that you created the AML workspace in. You can check this by running: az account show --query id Create an AML Compute cluster or AML Compute Instance . You will need the --name that you specified during compute cluster/instance creation to configure the .env.sh file to run the AML tests.","title":"Azure Machine Learning (AML) workspace and compute"},{"location":"getting-started/first-notebook/","text":"Clone the Sample Repo \u00b6 To get started, first you'll need to clone our sample repo. To do so, execute the following command: git clone https://github.com/SAME-Project/SAME-samples.git Next, change into the hello-world directory: cd SAME-samples cd 01 -hello-world Finally, deploy your notebook to your environment. If you are using kubeflow, you would execute the following command: same program run This command converts the notebook into a single python script, and deploys it to your previously configured Kubeflow. In the Kubeflow UI, click on Pipelines -> Experiments to see your runs!","title":"First Notebook"},{"location":"getting-started/first-notebook/#clone-the-sample-repo","text":"To get started, first you'll need to clone our sample repo. To do so, execute the following command: git clone https://github.com/SAME-Project/SAME-samples.git Next, change into the hello-world directory: cd SAME-samples cd 01 -hello-world Finally, deploy your notebook to your environment. If you are using kubeflow, you would execute the following command: same program run This command converts the notebook into a single python script, and deploys it to your previously configured Kubeflow. In the Kubeflow UI, click on Pipelines -> Experiments to see your runs!","title":"Clone the Sample Repo"},{"location":"getting-started/importing-packages/","text":"Background around Python Packages in Notebooks \u00b6 The SAME SDK helps data scientists avoid one of the most common problems in the development, forgetting to include their dependencies as part of their deployments. For example, it is very common to do something like the following when running a notebook: import foo import bar.qaz from qux import moo ! pip install baz All of the above work well when run locally, but if a data scientist is not diligent, and update their requirements.txt file (and commit that update to the code base), when run in prodcution, the code will not work (normally showing an error like \"module not found\"). Apart from just being annoying, this could also take a long time to detect, due to the length of time before pipeline jobs are executed. To aleviate this, the same-sdk enables both local import (using caching and cleaner outputs) and keeps the environment file in sync. Importing Packages via SAME \u00b6 To use same in a notebook, first one has to install it: pip install same-sdk Then, inside the notebook, instead of doing import same , a data scientist would execute: same . import ( \"package_name\" ) This will load the module into python's sys.modules . Later, when same program run is executed, it updates the environment.yaml file to the latest for all system modules for the entire notebook, and uses that file as the record for injecting requirements to the back end system.","title":"Importing Packages"},{"location":"getting-started/importing-packages/#background-around-python-packages-in-notebooks","text":"The SAME SDK helps data scientists avoid one of the most common problems in the development, forgetting to include their dependencies as part of their deployments. For example, it is very common to do something like the following when running a notebook: import foo import bar.qaz from qux import moo ! pip install baz All of the above work well when run locally, but if a data scientist is not diligent, and update their requirements.txt file (and commit that update to the code base), when run in prodcution, the code will not work (normally showing an error like \"module not found\"). Apart from just being annoying, this could also take a long time to detect, due to the length of time before pipeline jobs are executed. To aleviate this, the same-sdk enables both local import (using caching and cleaner outputs) and keeps the environment file in sync.","title":"Background around Python Packages in Notebooks"},{"location":"getting-started/importing-packages/#importing-packages-via-same","text":"To use same in a notebook, first one has to install it: pip install same-sdk Then, inside the notebook, instead of doing import same , a data scientist would execute: same . import ( \"package_name\" ) This will load the module into python's sys.modules . Later, when same program run is executed, it updates the environment.yaml file to the latest for all system modules for the entire notebook, and uses that file as the record for injecting requirements to the back end system.","title":"Importing Packages via SAME"},{"location":"getting-started/installing/","text":"Installing SAME \u00b6 Test drive \u00b6 If you want to try SAME without a Kubernetes cluster with Kubeflow installed, click the following Combinator.ml link, open the Kubeflow UI, then run through the rest of the Tutorial in the \"SSH\" tab, following the Ubuntu instructions. Launch Test Drive System Requirements \u00b6 Python >=3.8, <3.11, with pip Ubuntu/Debian: sudo apt update && sudo apt install -y python3-pip Install SAME using pip \u00b6 Before installing the pip package, using a virtual environment is highly recommended to isolate package installation from the system. (You can skip this step in the test drive, since the test drive VM is already isolated.) SAME project is available through PyPI : pip3 install sameproject Verify installation \u00b6 Validate successful installation by running same version . Output should look similar to below same version 0 .1.4 Connecting to a Workflow engine \u00b6 To run SAME, you will need a workflow engine to connect to. We support a variety of workflow engines, but recommend that, for now, you connect to one that is dedicated to SAME exclusively. If you are using the Combinator.ml test drive , you can skip this section as Kubernetes and Kubeflow is already configured in your test drive VM. Prerequisites \u00b6 Kubernetes cluster running locally or in a cloud environment kubectl installed Install Kubeflow \u00b6 Verify that correct Kubernetes cluster has been configured with the desired kubectl context set as the default. kubectl config current-context Set KUBECONFIG environment variable. While SAME can operate without setting the environment variable, some tools may expect this to be set. export KUBECONFIG = \"~/.kube/config\" If you use a non- bash shell, you may need to spell this command to set an environment variable differently. Install Kubeflow on Kubernetes Follow the Kubeflow installation instructions , or try the test drive if this is too much hard work. Next \u00b6 You're done setting up SAME and are now ready to execute! The default execution uses Kubeflow (either locally or in the cloud). Use the -t flag to set another target. To try out an example, check out the hello-world example from First Notebook .","title":"Installing"},{"location":"getting-started/installing/#installing-same","text":"","title":"Installing SAME"},{"location":"getting-started/installing/#test-drive","text":"If you want to try SAME without a Kubernetes cluster with Kubeflow installed, click the following Combinator.ml link, open the Kubeflow UI, then run through the rest of the Tutorial in the \"SSH\" tab, following the Ubuntu instructions. Launch Test Drive","title":"Test drive"},{"location":"getting-started/installing/#system-requirements","text":"Python >=3.8, <3.11, with pip Ubuntu/Debian: sudo apt update && sudo apt install -y python3-pip","title":"System Requirements"},{"location":"getting-started/installing/#install-same-using-pip","text":"Before installing the pip package, using a virtual environment is highly recommended to isolate package installation from the system. (You can skip this step in the test drive, since the test drive VM is already isolated.) SAME project is available through PyPI : pip3 install sameproject","title":"Install SAME using pip"},{"location":"getting-started/installing/#verify-installation","text":"Validate successful installation by running same version . Output should look similar to below same version 0 .1.4","title":"Verify installation"},{"location":"getting-started/installing/#connecting-to-a-workflow-engine","text":"To run SAME, you will need a workflow engine to connect to. We support a variety of workflow engines, but recommend that, for now, you connect to one that is dedicated to SAME exclusively. If you are using the Combinator.ml test drive , you can skip this section as Kubernetes and Kubeflow is already configured in your test drive VM.","title":"Connecting to a Workflow engine"},{"location":"getting-started/installing/#prerequisites","text":"Kubernetes cluster running locally or in a cloud environment kubectl installed","title":"Prerequisites"},{"location":"getting-started/installing/#install-kubeflow","text":"Verify that correct Kubernetes cluster has been configured with the desired kubectl context set as the default. kubectl config current-context Set KUBECONFIG environment variable. While SAME can operate without setting the environment variable, some tools may expect this to be set. export KUBECONFIG = \"~/.kube/config\" If you use a non- bash shell, you may need to spell this command to set an environment variable differently. Install Kubeflow on Kubernetes Follow the Kubeflow installation instructions , or try the test drive if this is too much hard work.","title":"Install Kubeflow"},{"location":"getting-started/installing/#next","text":"You're done setting up SAME and are now ready to execute! The default execution uses Kubeflow (either locally or in the cloud). Use the -t flag to set another target. To try out an example, check out the hello-world example from First Notebook .","title":"Next"}]}